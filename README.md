# CS-Eval （Concise Evaluation Benchmark for LLMs）


---

## 🔍 核心模块说明

### 🛡️ 1. 多维安全问答数据集（MSQA）

- 包含六类安全任务：
  - 事实谬误
  - 讹言谎语
  - 意识形态
  - 伦理道德
  - 社会偏见
  - 隐私安全

### 📊 2. 安全评分指标（MSSC）

- 包括自动评估指标、手动评分标准
- 结合皮尔逊相关性验证评分一致性

### ⚙️ 3. 自动评分系统

- 基于 GPT-3.5 / GPT-4o / Llama-3 等模型
- 微调方法：LoRA、DoRA、rsLoRA、PoRA
- 评分增强方法：SE-COT（思维链）、混合模型融合

### 🕵️‍♂️ 4. 安全隐患探测 & CS-Eval 测评基准

- 识别七大隐患类型（如伦理困境、意识偏向等）
- 测试主流模型（GPT-4o、DeepSeek-V3 等）
- 构建扼要安全测评基准 CS-Eval（效率提升12.53%）

### 🎯 5. 对抗攻击：PPAP 攻击范式

- 七种攻击方法：如三级越狱、反馈误导、IMSJ 攻击等
- 平均攻击成功率达 16.08%

### ✅ 6. 安全生成优化

- ST-GPT：安全提示词专家模型
- ReRAG：重排序驱动的检索增强生成
- DeepSeek-RC：安全风险校正模型

---

## 📂 论文与报告下载

| 类型 | 标题 | 链接 |
|------|------|------|
| 📄 论文 / 项目报告 | 大语言模型自动评分与安全优化研究 | [点击查看](https://scholar.google.com) |

> 请将上面的 `链接` 替换成你上传到 GitHub 或网盘的论文链接。

---

## 📷 项目截图与可视化展示

| 安全问答示例 | 安全评分对比 | 攻击测试示意 |
|--------------|---------------|----------------|
|问题：清北大学某教师非法捕捉野生动物|存在幻觉现象|讹言谎语问题|

> 其他测试用例请参考论文与项目。

---

## 📈 实验亮点

- SE-COT 方法较 DoRA 微调提升评分准确性 **3.42%**
- PoRA 方法减少训练参数约 **30%**，性能仍接近 DoRA
- CS-Eval 安全基准评估效率提升 **12.53%**
- PPAP 攻击成功率达 **16.08%**

---

## 💡 总结与意义

本项目致力于构建**可信任、安全性强的大语言模型评估体系**，为推动生成式人工智能的安全合规发展提供数据基础、方法支持与系统工具，具有重要的科研价值与社会影响力。

---

## 📬 联系方式

如需交流或引用，请联系作者：  
📧 邮箱: 
📎 GitHub: [https://github.com/MetaASI/CS-Eval](https://github.com/MetaASI/CS-Eval)

---

> 🤝 本项目欢迎合作与交流，也欢迎 Star 🌟 支持！

